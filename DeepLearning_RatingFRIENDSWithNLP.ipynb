{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0bf2b5",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-libraries\" data-toc-modified-id=\"Load-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load libraries</a></span></li><li><span><a href=\"#Import-X_transformed-and-y\" data-toc-modified-id=\"Import-X_transformed-and-y-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import X_transformed and y</a></span></li><li><span><a href=\"#Selecting-X-columns\" data-toc-modified-id=\"Selecting-X-columns-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Selecting X columns</a></span></li><li><span><a href=\"#Splitting-data\" data-toc-modified-id=\"Splitting-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Splitting data</a></span></li><li><span><a href=\"#Deeplearning\" data-toc-modified-id=\"Deeplearning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Deeplearning</a></span></li><li><span><a href=\"#Running-baseline-model\" data-toc-modified-id=\"Running-baseline-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Running baseline model</a></span></li><li><span><a href=\"#Re-running-baseline-model-with-data-preparation-(scaling)\" data-toc-modified-id=\"Re-running-baseline-model-with-data-preparation-(scaling)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Re-running baseline model with data preparation (scaling)</a></span><ul class=\"toc-item\"><li><span><a href=\"#StandardScaler\" data-toc-modified-id=\"StandardScaler-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>StandardScaler</a></span></li><li><span><a href=\"#MinMaxScaler\" data-toc-modified-id=\"MinMaxScaler-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>MinMaxScaler</a></span></li></ul></li><li><span><a href=\"#Tuning-layers-and-number-of-neurons-with-baseline\" data-toc-modified-id=\"Tuning-layers-and-number-of-neurons-with-baseline-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Tuning layers and number of neurons with baseline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Smaller-network\" data-toc-modified-id=\"Smaller-network-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Smaller network</a></span></li><li><span><a href=\"#Larger-network\" data-toc-modified-id=\"Larger-network-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Larger network</a></span></li><li><span><a href=\"#small-with-minmax-and-0.1%-dropout\" data-toc-modified-id=\"small-with-minmax-and-0.1%-dropout-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>small with minmax and 0.1% dropout</a></span></li><li><span><a href=\"#Batch-normalization\" data-toc-modified-id=\"Batch-normalization-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Batch normalization</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e12dc",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5b8b5c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import  LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# deeplearning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f27d6e",
   "metadata": {},
   "source": [
    "### Import X_transformed and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cfe460be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = pd.read_csv(\"X_transformed.csv\")\n",
    "\n",
    "y = pd.read_csv(\"y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a2b08",
   "metadata": {},
   "source": [
    "### Selecting X columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5c5ece9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lines = X_transformed[['joey_lines', 'chandler_lines', 'ross_lines', 'monica_lines', 'rachel_lines', 'phoebe_lines', 'janice_lines','Gunther_lines', 'ugly_naked_guy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd511ae",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "5cd8db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lines, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f3a1c",
   "metadata": {},
   "source": [
    "### Running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "dca343cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 47.22% (7.11%)\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_dim=9, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "# estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, verbose=0)))\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(estimator, X_lines, y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5552de",
   "metadata": {},
   "source": [
    "### Re-running baseline model with data preparation (scaling)\n",
    "#### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "540a2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 43.54% (8.97%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "# estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486e9f5",
   "metadata": {},
   "source": [
    "#### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "a79a78f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 50.91% (9.87%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eba236",
   "metadata": {},
   "source": [
    "The achieved results were:\n",
    "- Baseline: 47.22%\n",
    "- StandardScaler: 43.54%\n",
    "- MinMaxScaler: 50.91%\n",
    "\n",
    "Will follow through with model that has the minmax scaler. I plan to alter the size of the network to see if I can increase the cv accuracy.\n",
    "\n",
    "### Tuning layers and number of neurons with baseline\n",
    "#### Smaller network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "f102d5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 55.45% (7.31%)\n"
     ]
    }
   ],
   "source": [
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 5 = rounded(9/2)\n",
    "    model.add(Dense(5, input_dim=9, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33acd8e",
   "metadata": {},
   "source": [
    "#### Larger network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "ef1324a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 47.22% (7.70%)\n"
     ]
    }
   ],
   "source": [
    "def create_larger():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 5 = rounded(9/2)\n",
    "    model.add(Dense(9, input_dim=9, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc362c87",
   "metadata": {},
   "source": [
    "The achieved results were:\n",
    "- Base with minmax: 50.91%\n",
    "- Smaller base with minmax: 55.45% \n",
    "- Larger base with minmax: 47.22%\n",
    "\n",
    "Seems like shrinking the model a bit with the minmax scaler did me quite the favour. Will follow through with this model and see how **dropout** affects the model.\n",
    "\n",
    "#### small with minmax and 0.1% dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "138600a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 48.62% (5.46%)\n"
     ]
    }
   ],
   "source": [
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 5 = rounded(9/2)\n",
    "    model.add(Dense(5, input_dim=9, activation='relu'))\n",
    "    model.add(layers.Dropout(0.1)) # Set 10% of the nodes to 0.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "35eaa20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 53.57% (11.11%)\n"
     ]
    }
   ],
   "source": [
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 5 = rounded(9/2)\n",
    "    model.add(Dense(5, input_dim=9, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2)) # Set 10% of the nodes to 0.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "3b71ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 49.51% (5.12%)\n"
     ]
    }
   ],
   "source": [
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 5 = rounded(9/2)\n",
    "    model.add(Dense(5, input_dim=9, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3)) # Set 10% of the nodes to 0.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060086d2",
   "metadata": {},
   "source": [
    "Tested dropouts ranging from 10-30% but seemed to produce lower cv accuracies than the smaller baseline model with a minmax scaler. \n",
    "Will test batch normalization with the hopes of improving the cv accuracy a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8f5f9",
   "metadata": {},
   "source": [
    "#### Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "23411ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 46.30% (5.39%)\n"
     ]
    }
   ],
   "source": [
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 5 = rounded(9/2)\n",
    "    model.add(Dense(5, input_dim=9, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "# estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(pipeline, X_lines, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ea149",
   "metadata": {},
   "source": [
    "Batch normalization didn't improve accuracy.\n",
    "\n",
    "### Summary\n",
    "To sum things up, the model with the highest 5 fold cv accuracy (55.45%) was the smaller model with the minmax scalar. This was a great way to get a sense of how altering a model affects its cv accuracy. That being said, DeepLearning is definitely something I need to deep (pun intended) dive into and explore the possibilities. It was also great to see the implementation of deeplearning in python rather than coding the algorithms from scratch on MATLAB (aaah school).\n",
    "\n",
    "I welcome suggestions and/or resources on how to improve my code and thinking process. Thank you in advance! :)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
